{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting familiar with the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, roc_curve, auc\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from tempfile import mkdtemp\n",
    "from joblib import Memory\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "data = pd.read_csv('./Project1-Classification.csv')\n",
    "print('number of rows:',data.shape[0])\n",
    "print('number of columns:',data.shape[1])\n",
    "data.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_count = []\n",
    "for i in range(data.shape[0]):\n",
    "    text = data.iloc[i].at['full_text']\n",
    "    count = sum(1 for char in text if char.isalnum())\n",
    "    text_count.append(count)\n",
    "\n",
    "plt.hist(text_count,bins=50)\n",
    "plt.xlabel('Number of characters in text',fontsize=14)\n",
    "plt.ylabel('Frequency',fontsize=14)\n",
    "plt.xlim((0,15000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "res = dict(Counter(data['leaf_label']))\n",
    "x = list(res.keys())\n",
    "y = list(res.values())\n",
    "plt.bar(x,y)\n",
    "for a,b in zip(x,y):\n",
    "    plt.text(a,b,b,ha='center',va='bottom')\n",
    "plt.xlabel('Classes of leaf_label',fontsize=14)\n",
    "plt.ylabel('Number of samples',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dict(Counter(data['root_label']))\n",
    "x = list(res.keys())\n",
    "y = list(res.values())\n",
    "plt.bar(x,y)\n",
    "for a,b in zip(x,y):\n",
    "    plt.text(a,b,b,ha='center',va='bottom')\n",
    "plt.xlabel('Classes of root_label',fontsize=14)\n",
    "plt.ylabel('Number of samples',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quesion 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the entire dataset into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train-test data\n",
    "train, test = train_test_split(data[[\"full_text\",\"root_label\"]], test_size=0.2, random_state=42)\n",
    "print('train sample number: ',len(train))\n",
    "print('test sample number: ',len(test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(text):\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    texter = re.sub(r\"<br />\", \" \", text)\n",
    "    texter = re.sub(r\"&quot;\", \"\\\"\",texter)\n",
    "    texter = re.sub('&#39;', \"\\\"\", texter)\n",
    "    texter = re.sub('\\n', \" \", texter)\n",
    "    texter = re.sub(' u ',\" you \", texter)\n",
    "    texter = re.sub('`',\"\", texter)\n",
    "    texter = re.sub(' +', ' ', texter)\n",
    "    texter = re.sub(r\"(!)\\1+\", r\"!\", texter)\n",
    "    texter = re.sub(r\"(\\?)\\1+\", r\"?\", texter)\n",
    "    texter = re.sub('&amp;', 'and', texter)\n",
    "    texter = re.sub('\\r', ' ',texter)\n",
    "    clean = re.compile('<.*?>')\n",
    "    texter = texter.encode('ascii', 'ignore').decode('ascii')\n",
    "    texter = re.sub(clean, '', texter)\n",
    "    if texter == \"\":\n",
    "        texter = \"\"\n",
    "    return texter\n",
    "\n",
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data\n",
    "\n",
    "train.applymap(clean)\n",
    "test.applymap(clean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing and produce TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tag(tag):\n",
    "    tag_dict = {'NN':'n', 'JJ':'a','VB':'v', 'RB':'r'}\n",
    "    if tag in tag_dict.keys():\n",
    "        return tag_dict[tag]\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "def lemmatizer(text):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    words_list = []\n",
    "    for word,tag in pos_tag(word_tokenize(text)):\n",
    "        words_list.append(lemma.lemmatize(word=word.lower(),pos=transform_tag(tag)))\n",
    "    return words_list\n",
    "\n",
    "\n",
    "# lemmatize text\n",
    "for i in range(len(train)):\n",
    "    text = train.iloc[i].at['full_text']\n",
    "    lemma_words = lemmatizer(text)\n",
    "    lemma_words = [i for i in lemma_words if i not in string.punctuation] \n",
    "    lemma_words = [i for i in lemma_words if not i.isdigit()]\n",
    "    train.iloc[i].at['full_text'] = ' '.join(lemma_words)\n",
    "\n",
    "\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=3, stop_words='english')\n",
    "X_train_counts = vectorizer.fit_transform(train['full_text'])\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_counts) # making the tfidf train matrix\n",
    "\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test)):\n",
    "    text = test.iloc[i].at['full_text']\n",
    "    lemma_words = lemmatizer(text)\n",
    "    lemma_words = [i for i in lemma_words if i not in string.punctuation] \n",
    "    lemma_words = [i for i in lemma_words if not i.isdigit()]\n",
    "    test.iloc[i].at['full_text'] = ' '.join(lemma_words) \n",
    "\n",
    "X_test_counts = vectorizer.transform(test['full_text'])\n",
    "X_test_tfidf = tfidf.transform(X_test_counts)\n",
    "X_test_tfidf.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSI & NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSI - explained_variance_ratio plot\n",
    "\n",
    "k = [1, 10, 50, 100, 200, 500, 1000, 2000]\n",
    "explained_variance_ratio = []\n",
    "train_list_LSI = []\n",
    "test_list_LSI = []\n",
    "for i in range(len(k)):\n",
    "    LSI_model = TruncatedSVD(n_components=k[i], random_state=42)\n",
    "    X_train_LSI = LSI_model.fit_transform(X_train_tfidf)\n",
    "    X_test_LSI = LSI_model.transform(X_test_tfidf)\n",
    "    train_list_LSI.append(X_train_LSI)\n",
    "    test_list_LSI.append(X_test_LSI)\n",
    "    explained_variance_ratio.append(sum(LSI_model.explained_variance_ratio_))\n",
    "\n",
    "plt.plot(k,explained_variance_ratio) # explained_variance_ratio plot with different k\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF error\n",
    "\n",
    "nmf = NMF(n_components=50, init='random', random_state=42)\n",
    "train_matrix_NMF = nmf.fit_transform(X_train_tfidf) # performing NMF on the tfidf train matrix\n",
    "test_matrix_NMF = nmf.transform(X_test_tfidf) # performing NMF on the tfidf test matrix\n",
    "H = nmf.components_\n",
    "print(\"Error for NMF: \", np.sum(np.array(X_train_tfidf - train_matrix_NMF.dot(H))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI error\n",
    "\n",
    "LSI_model = TruncatedSVD(n_components=50, random_state=42)\n",
    "train_matrix_LSI = LSI_model.fit_transform(X_train_tfidf)\n",
    "test_matrix_LSI = LSI_model.transform(X_test_tfidf)\n",
    "print(\"Error for LSI: \", np.sum(np.array(X_train_tfidf - train_matrix_LSI @ LSI_model.components_)**2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_roc(fpr, tpr):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "\n",
    "    ax.plot(fpr, tpr, lw=2, label= 'area under curve = %0.4f' % roc_auc)\n",
    "\n",
    "    ax.grid(color='0.7', linestyle='--', linewidth=1)\n",
    "\n",
    "    ax.set_xlim([-0.1, 1.1])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate',fontsize=15)\n",
    "    ax.set_ylabel('True Positive Rate',fontsize=15)\n",
    "\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    for label in ax.get_xticklabels()+ax.get_yticklabels():\n",
    "        label.set_fontsize(15)\n",
    "\n",
    "def plot_confusion_matrix(matrix,title):\n",
    "    f,ax = plt.subplots()\n",
    "    sns.heatmap(matrix,annot=True,ax=ax,fmt='.20g') #画热力图\n",
    "\n",
    "    ax.set_title('confusion matrix of ' + title,fontsize=13) #标题\n",
    "    ax.set_xlabel('predict',fontsize=13) #x轴\n",
    "    ax.set_ylabel('true',fontsize=13) #y轴\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVM: soft margin V.S. hard margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard-margin SVM, soft-margin SVM and harder-margin SVM performances contract\n",
    "\n",
    "hard_LSVC = LinearSVC(C=1000,random_state=42)\n",
    "soft_LSVC = LinearSVC(C=0.0001,random_state=42)\n",
    "harder_LSVC = LinearSVC(C=100000,random_state=42)\n",
    "\n",
    "train_label = [1 if i == 'sports' else 0 for i in train['root_label']]\n",
    "test_label = [1 if i == 'sports' else 0 for i in test['root_label']]\n",
    "\n",
    "hard_LSVC.fit(train_matrix_LSI,train_label)\n",
    "soft_LSVC.fit(train_matrix_LSI,train_label)\n",
    "harder_LSVC.fit(train_matrix_LSI,train_label)\n",
    "\n",
    "hard_pred = hard_LSVC.predict(test_matrix_LSI)\n",
    "soft_pred = soft_LSVC.predict(test_matrix_LSI)\n",
    "harder_pred = hard_LSVC.predict(test_matrix_LSI)\n",
    "\n",
    "hard_confusion_matrix = confusion_matrix(test_label,hard_pred)\n",
    "soft_confusion_matrix = confusion_matrix(test_label,soft_pred)\n",
    "harder_confusion_matrix = confusion_matrix(test_label,harder_pred)\n",
    "\n",
    "print('hard_confusion_matrix:\\n',hard_confusion_matrix)\n",
    "print('soft_confusion_matrix:\\n',soft_confusion_matrix)\n",
    "print('harder_confusion_matrix:\\n',harder_confusion_matrix)\n",
    "print('\\n')\n",
    "\n",
    "hard_acc = accuracy_score(test_label,hard_pred)\n",
    "soft_acc = accuracy_score(test_label,soft_pred)\n",
    "harder_acc = accuracy_score(test_label,harder_pred)\n",
    "\n",
    "print('hard acc: ',hard_acc)\n",
    "print('soft acc: ',soft_acc)\n",
    "print('harder acc: ',harder_acc)\n",
    "print('\\n')\n",
    "\n",
    "hard_recall = recall_score(test_label,hard_pred)\n",
    "soft_recall = recall_score(test_label,soft_pred)\n",
    "harder_recall = recall_score(test_label,harder_pred)\n",
    "\n",
    "print('hard recall: ',hard_recall)\n",
    "print('soft recall: ',soft_recall)\n",
    "print('harder recall: ',harder_recall)\n",
    "print('\\n')\n",
    "\n",
    "hard_precision = precision_score(test_label,hard_pred)\n",
    "soft_precision = precision_score(test_label,soft_pred)\n",
    "harder_precision = precision_score(test_label,harder_pred)\n",
    "\n",
    "print('hard precision: ',hard_precision)\n",
    "print('soft precision: ',soft_precision)\n",
    "print('harder precision: ',harder_precision)\n",
    "print('\\n')\n",
    "\n",
    "hard_f1 = f1_score(test_label,hard_pred)\n",
    "soft_f1 = f1_score(test_label,soft_pred)\n",
    "harder_f1 = f1_score(test_label,harder_pred)\n",
    "\n",
    "print('hard f1: ',hard_f1)\n",
    "print('soft f1: ',soft_f1)\n",
    "print('harder f1: ',harder_f1)\n",
    "print('\\n')\n",
    "\n",
    "fpr_hard, tpr_hard, _ = roc_curve(test_label,hard_LSVC.decision_function(test_matrix_LSI))\n",
    "fpr_soft, tpr_soft, _ = roc_curve(test_label,soft_LSVC.decision_function(test_matrix_LSI))\n",
    "fpr_harder, tpr_harder, _ = roc_curve(test_label,harder_LSVC.decision_function(test_matrix_LSI))\n",
    "\n",
    "plot_roc(fpr_hard, tpr_hard)\n",
    "plot_roc(fpr_soft, tpr_soft)\n",
    "plot_roc(fpr_harder, tpr_harder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(confusion_matrix(test_label,hard_pred),'SVM with gamma = 1000')\n",
    "plot_confusion_matrix(confusion_matrix(test_label,soft_pred),'SVM with gamma = 0.0001')\n",
    "plot_confusion_matrix(confusion_matrix(test_label,harder_pred),'SVM with gamma = 100000')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation to find best gamma for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation for choosing best C(gamma) for LSVC model using gridsearch\n",
    "\n",
    "svc_C = LinearSVC(random_state=42)\n",
    "hyperparameter = {'C':[0.001,0.01,0.1,1,10,1e2,1e3,1e4,1e5,1e6]}\n",
    "svc_C_clf = GridSearchCV(svc_C,hyperparameter,cv=5,scoring='accuracy')\n",
    "svc_C_clf.fit(train_matrix_LSI,train_label)\n",
    "\n",
    "print('best hyperparameter gamma: ',svc_C_clf.best_estimator_.C)\n",
    "best_pred = svc_C_clf.best_estimator_.predict(test_matrix_LSI)\n",
    "\n",
    "print('best confusion matrix: \\n',confusion_matrix(test_label,best_pred))\n",
    "print('best acc: ',accuracy_score(test_label,best_pred))\n",
    "print('best recall: ',recall_score(test_label,best_pred))\n",
    "print('best precision: ',precision_score(test_label,best_pred))\n",
    "print('best f1: ',f1_score(test_label,best_pred))\n",
    "\n",
    "fpr_best, tpr_best, _ = roc_curve(test_label,svc_C_clf.best_estimator_.decision_function(test_matrix_LSI))\n",
    "plot_roc(fpr_best, tpr_best)\n",
    "plot_confusion_matrix(confusion_matrix(test_label,best_pred),'best SVM')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression without penalty for classification\n",
    "clf_lr = LogisticRegression(penalty='none',random_state=42)\n",
    "clf_lr.fit(train_matrix_LSI,train_label)\n",
    "\n",
    "lr_pred = clf_lr.predict(test_matrix_LSI)\n",
    "\n",
    "print('lr confusion matrix: \\n',confusion_matrix(test_label,lr_pred))\n",
    "print('lr acc: ',accuracy_score(test_label,lr_pred))\n",
    "print('lr recall: ',recall_score(test_label,lr_pred))\n",
    "print('lr precision: ',precision_score(test_label,lr_pred))\n",
    "print('lr f1: ',f1_score(test_label,lr_pred))\n",
    "\n",
    "fpr_lr, tpr_lr, _ = roc_curve(test_label,clf_lr.decision_function(test_matrix_LSI))\n",
    "plot_roc(fpr_lr, tpr_lr)\n",
    "plot_confusion_matrix(confusion_matrix(test_label,lr_pred),'LogisticRegression w/o regularization')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 and L2 logistic regression and find their best hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best hyperparameter for logistic regression with l1 and l2 penalty respectively\n",
    "lr_l1 = LogisticRegression(solver='liblinear',penalty='l1',random_state=42)\n",
    "hyperparameter = {'C':[1e-5,1e-4,1e-3,1e-2,1e-1,1,1e1,1e2,1e3,1e4,1e5]}\n",
    "lr_l1_clf = GridSearchCV(lr_l1,hyperparameter,cv=5,scoring='accuracy')\n",
    "lr_l1_clf.fit(train_matrix_NMF,train_label)\n",
    "\n",
    "print('best regularization strength for l1: ',lr_l1_clf.best_estimator_.C)\n",
    "print(accuracy_score(test_label,lr_l1_clf.best_estimator_.predict(test_matrix_NMF)))\n",
    "\n",
    "lr_l2 = LogisticRegression(solver='liblinear',penalty='l2',random_state=42)\n",
    "hyperparameter = {'C':[1e-5,1e-4,1e-3,1e-2,1e-1,1,1e1,1e2,1e3,1e4,1e5]}\n",
    "lr_l2_clf = GridSearchCV(lr_l2,hyperparameter,cv=5,scoring='accuracy')\n",
    "lr_l2_clf.fit(train_matrix_NMF,train_label)\n",
    "\n",
    "print('best regularization strength for l2: ',lr_l2_clf.best_estimator_.C)\n",
    "print(accuracy_score(test_label,lr_l1_clf.best_estimator_.predict(test_matrix_NMF)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare 3 logistic regression classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_l1 = lr_l1_clf.best_estimator_.predict(test_matrix_NMF)\n",
    "pred_l2 = lr_l2_clf.best_estimator_.predict(test_matrix_NMF)\n",
    "\n",
    "print('lr_no_penalty acc: ',accuracy_score(test_label,lr_pred))\n",
    "print('lr_l1 acc: ',accuracy_score(test_label,pred_l1))\n",
    "print('lr_l2 acc: ',accuracy_score(test_label,pred_l2))\n",
    "\n",
    "print('lr_no_penalty recall: ',recall_score(test_label,lr_pred))\n",
    "print('lr_l1 recall: ',recall_score(test_label,pred_l1))\n",
    "print('lr_l2 recall: ',recall_score(test_label,pred_l2))\n",
    "\n",
    "print('lr_no_penalty precision: ',precision_score(test_label,lr_pred))\n",
    "print('lr_l1 precision: ',precision_score(test_label,pred_l1))\n",
    "print('lr_l2 precision: ',precision_score(test_label,pred_l2))\n",
    "\n",
    "print('lr_no_penalty f1: ',f1_score(test_label,lr_pred))\n",
    "print('lr_l1 f1: ',f1_score(test_label,pred_l1))\n",
    "print('lr_l2 f1: ',f1_score(test_label,pred_l2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayes for classification\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(train_matrix_LSI,train_label)\n",
    "pred_gnb = gnb.predict(test_matrix_LSI)\n",
    "\n",
    "print('gnb confusion matrix: \\n',confusion_matrix(test_label,pred_gnb))\n",
    "print('gnb acc: ',accuracy_score(test_label,pred_gnb))\n",
    "print('gnb recall: ',recall_score(test_label,pred_gnb))\n",
    "print('gnb precision: ',precision_score(test_label,pred_gnb))\n",
    "print('gnb f1: ',f1_score(test_label,pred_gnb))\n",
    "\n",
    "fpr_gnb, tpr_gnb, _ = roc_curve(test_label,gnb.predict_proba(test_matrix_LSI)[:,1])\n",
    "plot_roc(fpr_lr, tpr_lr)\n",
    "plot_confusion_matrix(confusion_matrix(test_label,pred_gnb),'Naive Bayes Model')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline and gridsearch to find best hyperparameters combination \n",
    "def use_lemma(text):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    words_list = []\n",
    "    for word,tag in pos_tag(word_tokenize(text)):\n",
    "        words_list.append(lemma.lemmatize(word=word.lower(),pos=transform_tag(tag)))\n",
    "    return words_list\n",
    "\n",
    "def use_stem(text):\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    word_list = []\n",
    "    for word in word_tokenize(text):\n",
    "        word_list.append(ps.stem(word.lower()))\n",
    "\n",
    "cachedir = mkdtemp()\n",
    "memory = Memory(location=cachedir, verbose=10)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('Feature_Extraction',TfidfVectorizer(stop_words='english')),\n",
    "    ('Dimensionality_Reduction',None),\n",
    "    ('Classifier',None)],memory=memory)\n",
    "\n",
    "param_grid = {\n",
    "    'Feature_Extraction__min_df': (3,5),\n",
    "    'Feature_Extraction__analyzer':(use_lemma,use_stem),\n",
    "    'Dimensionality_Reduction':(TruncatedSVD(n_components=5,random_state=42), TruncatedSVD(n_components=30,random_state=42), \n",
    "                TruncatedSVD(n_components=80,random_state=42), NMF(n_components=5,init='random', random_state=42),\n",
    "                NMF(n_components=30,init='random', random_state=42), NMF(n_components=80,init='random', random_state=42)),\n",
    "    'Classifier':(LinearSVC(C=svc_C_clf.best_estimator_.C,random_state=42), LogisticRegression(solver='liblinear',penalty='l2',C=lr_l2_clf.best_estimator_.C,random_state=42),\n",
    "                LogisticRegression(solver='liblinear',penalty='l1',C=lr_l1_clf.best_estimator_.C,random_state=42), GaussianNB())\n",
    "    }\n",
    "\n",
    "grid = GridSearchCV(pipe,cv=5,param_grid=param_grid,scoring='accuracy')\n",
    "\n",
    "data = pd.read_csv('./Project1-Classification.csv')\n",
    "train, test = train_test_split(data[[\"full_text\",\"root_label\"]], test_size=0.2, random_state=42)\n",
    "train.applymap(clean)\n",
    "test.applymap(clean)\n",
    "\n",
    "grid.fit(train['full_text'], train['root_label'])\n",
    "print(\"Best score for pipeline: \", grid.best_score_)\n",
    "print(\"Best params for pipeline: \", grid.best_params_)\n",
    "print(\"Best estimator for pipeline: \", grid.best_estimator_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select top 5 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "li = np.array([i if str(i) != 'nan' else 0 for i in grid.cv_results_['mean_test_score']])\n",
    "arg = li.argsort()[-5:]\n",
    "for i in range(len(arg)):\n",
    "\n",
    "    print(grid.cv_results_['params'][arg[4-i]])\n",
    "    print(grid.cv_results_['mean_test_score'][arg[4-i]])\n",
    "    print('\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 2-5 models performances on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./Project1-Classification.csv')\n",
    "train, test = train_test_split(data[[\"full_text\",\"root_label\"]], test_size=0.2, random_state=42)\n",
    "train.applymap(clean)\n",
    "test.applymap(clean)\n",
    "\n",
    "train_c = train.copy()\n",
    "test_c  = test.copy()\n",
    "for i in range(len(train_c)):\n",
    "    text = train_c.iloc[i].at['full_text']\n",
    "    lemma_words = use_lemma(text)\n",
    "    train_c.iloc[i].at['full_text'] = ' '.join(lemma_words)\n",
    "\n",
    "for i in range(len(test_c)):\n",
    "    text = test.iloc[i].at['full_text']\n",
    "    lemma_words = use_lemma(text)\n",
    "    test_c.iloc[i].at['full_text'] = ' '.join(lemma_words)\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english',min_df=3)\n",
    "train_X =  tfidf.fit_transform(train_c['full_text'])\n",
    "test_X = tfidf.transform(test_c['full_text'])\n",
    "\n",
    "svd = TruncatedSVD(n_components=80, random_state=42)\n",
    "train_X = svd.fit_transform(train_X)\n",
    "test_X = svd.transform(test_X)\n",
    "\n",
    "train_Y = train_c['root_label']\n",
    "test_Y = test_c['root_label']\n",
    "\n",
    "lsvc = LogisticRegression(C=100000.0, penalty='l2', random_state=42, solver='liblinear')\n",
    "lsvc.fit(train_X,train_Y)\n",
    "pr = lsvc.predict(test_X)\n",
    "\n",
    "print('acc: ',accuracy_score(test_Y,pr))\n",
    "print('recall: ',recall_score(test_Y,pr, pos_label='sports'))\n",
    "print('precision: ',precision_score(test_Y,pr,pos_label='sports'))\n",
    "print('f1: ',f1_score(test_Y,pr,pos_label='sports'))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pre = grid.best_estimator_.predict(test['full_text'])\n",
    "\n",
    "print('best confusion matrix: \\n',confusion_matrix(test['root_label'],best_pre))\n",
    "print('best acc: ',accuracy_score(test['root_label'],best_pre))\n",
    "print('best recall: ',recall_score(test['root_label'],best_pre, pos_label='sports'))\n",
    "print('best precision: ',precision_score(test['root_label'],best_pre,pos_label='sports'))\n",
    "print('best f1: ',f1_score(test['root_label'],best_pre,pos_label='sports'))\n",
    "\n",
    "fpr_best, tpr_best, _ = roc_curve(test['root_label'],grid.best_estimator_.predict_proba(test['full_text'])[:,1],pos_label='sports')\n",
    "plot_roc(fpr_lr, tpr_lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes, OneVsOne SVM and OneVsRest SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayes classifier, one_vs_one classifier and one_vs_rest classifier performances compare\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "\n",
    "data = pd.read_csv('./Project1-Classification.csv')\n",
    "train, test = train_test_split(data[[\"full_text\",\"leaf_label\"]], test_size=0.2, random_state=42)\n",
    "train.applymap(clean)\n",
    "test.applymap(clean)\n",
    "\n",
    "for i in range(len(train)):\n",
    "    text = train.iloc[i].at['full_text']\n",
    "    lemma_words = lemmatizer(text)\n",
    "    lemma_words = [i for i in lemma_words if i not in string.punctuation] \n",
    "    lemma_words = [i for i in lemma_words if not i.isdigit()]\n",
    "    train.iloc[i].at['full_text'] = ' '.join(lemma_words)\n",
    "\n",
    "for i in range(len(test)):\n",
    "    text = test.iloc[i].at['full_text']\n",
    "    lemma_words = lemmatizer(text)\n",
    "    lemma_words = [i for i in lemma_words if i not in string.punctuation] \n",
    "    lemma_words = [i for i in lemma_words if not i.isdigit()]\n",
    "    test.iloc[i].at['full_text'] = ' '.join(lemma_words) \n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "vectorizer = CountVectorizer(min_df=3, stop_words='english')\n",
    "\n",
    "X_train_counts = vectorizer.fit_transform(train['full_text'])\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_counts)\n",
    "X_test_counts = vectorizer.transform(test['full_text'])\n",
    "X_test_tfidf = tfidf.transform(X_test_counts)\n",
    "\n",
    "LSI_model = TruncatedSVD(n_components=50, random_state=42)\n",
    "train_matrix_LSI = LSI_model.fit_transform(X_train_tfidf)\n",
    "test_matrix_LSI = LSI_model.transform(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train['leaf_label']\n",
    "test_label = test['leaf_label']\n",
    "\n",
    "map_row_to_class = {0:\"chess\", 1:\"cricket\", 2:\"hockey\", 3:\"soccer\",\n",
    "4:\"football\", 5:r\"%22forest%20fire%22\", 6:\"flood\", 7:\"earthquake\",\n",
    "8:\"drought\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_train = train.copy()\n",
    "copy_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_order = list(map_row_to_class.values())\n",
    "labels_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(train_matrix_LSI,train_label)\n",
    "gnb_pre = gnb.predict(test_matrix_LSI)\n",
    "\n",
    "print('gnb confusion matrix: \\n',confusion_matrix(test_label,gnb_pre,labels=labels_order))\n",
    "print('gnb acc: ',accuracy_score(test_label,gnb_pre))\n",
    "print('gnb recall: ',recall_score(test_label,gnb_pre,average='weighted',labels=labels_order))\n",
    "print('gnb precision: ',precision_score(test_label,gnb_pre,average='weighted',labels=labels_order))\n",
    "print('gnb f1: ',f1_score(test_label,gnb_pre,average='weighted',labels=labels_order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_vs_one = OneVsOneClassifier(LinearSVC(C = 100,random_state=42))\n",
    "one_vs_one.fit(train_matrix_LSI,train_label)\n",
    "one_pre = one_vs_one.predict(test_matrix_LSI)\n",
    "\n",
    "print('one_vs_one confusion matrix: \\n',confusion_matrix(test_label,one_pre,labels=labels_order))\n",
    "print('one_vs_one acc: ',accuracy_score(test_label,one_pre))\n",
    "print('one_vs_one recall: ',recall_score(test_label,one_pre,average='weighted',labels=labels_order))\n",
    "print('one_vs_one precision: ',precision_score(test_label,one_pre,average='weighted',labels=labels_order))\n",
    "print('one_vs_one f1: ',f1_score(test_label,one_pre,average='weighted',labels=labels_order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_vs_rest = OneVsRestClassifier(LinearSVC(C=100,random_state=42))\n",
    "one_vs_rest.fit(train_matrix_LSI,train_label)\n",
    "rest_pre = one_vs_rest.predict(test_matrix_LSI)\n",
    "\n",
    "print('one_vs_rest confusion matrix: \\n',confusion_matrix(test_label,rest_pre,labels=labels_order))\n",
    "print('one_vs_rest acc: ',accuracy_score(test_label,rest_pre))\n",
    "print('one_vs_rest recall: ',recall_score(test_label,rest_pre,average='weighted',labels=labels_order))\n",
    "print('one_vs_rest precision: ',precision_score(test_label,rest_pre,average='weighted',labels=labels_order))\n",
    "print('one_vs_rest f1: ',f1_score(test_label,rest_pre,average='weighted',labels=labels_order))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging classes\n",
    "for i in range(copy_train.shape[0]):\n",
    "    if copy_train.iloc[i].at['leaf_label'] == 'soccer':\n",
    "        copy_train.iloc[i].at['leaf_label'] = 'football'\n",
    "\n",
    "for i in range(copy_test.shape[0]):\n",
    "    if copy_test.iloc[i].at['leaf_label'] == 'soccer':\n",
    "        copy_test.iloc[i].at['leaf_label'] = 'football'\n",
    "\n",
    "merged_train_label = copy_train['leaf_label']\n",
    "merged_test_label = copy_test['leaf_label']\n",
    "\n",
    "merged_map_to_class = {0:\"chess\", 1:\"cricket\", 2:\"hockey\", 3:\"football\",\n",
    "4:r\"%22forest%20fire%22\", 5:\"flood\", 6:\"earthquake\",\n",
    "7:\"drought\"}\n",
    "\n",
    "merged_labels_order = list(merged_map_to_class.values())\n",
    "\n",
    "one_vs_one = OneVsOneClassifier(LinearSVC(C=100,random_state=42))\n",
    "one_vs_one.fit(train_matrix_LSI,merged_train_label)\n",
    "one_pre = one_vs_one.predict(test_matrix_LSI)\n",
    "\n",
    "print('one_vs_one confusion matrix: \\n',confusion_matrix(merged_test_label,one_pre,labels=merged_labels_order))\n",
    "print('one_vs_one acc: ',accuracy_score(merged_test_label,one_pre))\n",
    "print('one_vs_one recall: ',recall_score(merged_test_label,one_pre,average='weighted',labels=merged_labels_order))\n",
    "print('one_vs_one precision: ',precision_score(merged_test_label,one_pre,average='weighted',labels=merged_labels_order))\n",
    "print('one_vs_one f1: ',f1_score(merged_test_label,one_pre,average='weighted',labels=merged_labels_order))\n",
    "\n",
    "one_vs_rest = OneVsRestClassifier(LinearSVC(C=100,random_state=42))\n",
    "one_vs_rest.fit(train_matrix_LSI,merged_train_label)\n",
    "rest_pre = one_vs_rest.predict(test_matrix_LSI)\n",
    "\n",
    "print('one_vs_rest confusion matrix: \\n',confusion_matrix(merged_test_label,rest_pre,labels=merged_labels_order))\n",
    "print('one_vs_rest acc: ',accuracy_score(merged_test_label,rest_pre))\n",
    "print('one_vs_rest recall: ',recall_score(merged_test_label,rest_pre,average='weighted',labels=merged_labels_order))\n",
    "print('one_vs_rest precision: ',precision_score(merged_test_label,rest_pre,average='weighted',labels=merged_labels_order))\n",
    "print('one_vs_rest f1: ',f1_score(merged_test_label,rest_pre,average='weighted',labels=merged_labels_order))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use balanced class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolve class imbalance\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "one_vs_one = OneVsOneClassifier(LinearSVC(C=100,random_state=42,class_weight='balanced'))\n",
    "one_vs_one.fit(train_matrix_LSI,merged_train_label)\n",
    "one_pre = one_vs_one.predict(test_matrix_LSI)\n",
    "\n",
    "print('one_vs_one confusion matrix: \\n',confusion_matrix(merged_test_label,one_pre,labels=merged_labels_order))\n",
    "print('one_vs_one acc: ',accuracy_score(merged_test_label,one_pre))\n",
    "print('one_vs_one recall: ',recall_score(merged_test_label,one_pre,average='weighted',labels=merged_labels_order))\n",
    "print('one_vs_one precision: ',precision_score(merged_test_label,one_pre,average='weighted',labels=merged_labels_order))\n",
    "print('one_vs_one f1: ',f1_score(merged_test_label,one_pre,average='weighted',labels=merged_labels_order)) \n",
    "\n",
    "one_vs_rest = OneVsRestClassifier(LinearSVC(C=100,random_state=42,class_weight='balanced'))\n",
    "one_vs_rest.fit(train_matrix_LSI,merged_train_label)\n",
    "rest_pre = one_vs_rest.predict(test_matrix_LSI)\n",
    "\n",
    "print('one_vs_rest confusion matrix: \\n',confusion_matrix(merged_test_label,rest_pre,labels=merged_labels_order))\n",
    "print('one_vs_rest acc: ',accuracy_score(merged_test_label,rest_pre))\n",
    "print('one_vs_rest recall: ',recall_score(merged_test_label,rest_pre,average='weighted',labels=merged_labels_order))\n",
    "print('one_vs_rest precision: ',precision_score(merged_test_label,rest_pre,average='weighted',labels=merged_labels_order))\n",
    "print('one_vs_rest f1: ',f1_score(merged_test_label,rest_pre,average='weighted',labels=merged_labels_order))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLoVE + SVM pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove\n",
    "import ast\n",
    "\n",
    "train, test = train_test_split(data[[\"keywords\",\"root_label\"]], test_size=0.2, random_state=42)\n",
    "train.applymap(clean)\n",
    "test.applymap(clean)\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "res_acc = []\n",
    "dimension_of_glove = [50,100,200,300]\n",
    "for dim in dimension_of_glove:\n",
    "    embeddings_dict = {}\n",
    "    with open(\"./glove.6B.\"+ str(dim) + \"d.txt\", 'rb') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "\n",
    "    X = []\n",
    "\n",
    "    for i in range(train.shape[0]):\n",
    "        vect = [0 for k in range(dim)]\n",
    "        keywords_str = train.iloc[i].at['keywords']\n",
    "        keywords_list = ast.literal_eval(keywords_str)\n",
    "        length = 0\n",
    "        for j in range(len(keywords_list)):\n",
    "            if bytes(keywords_list[j],'utf-8') in embeddings_dict.keys():\n",
    "                vect += embeddings_dict[bytes(keywords_list[j],'utf-8')]\n",
    "                length += 1\n",
    "        vect /= length\n",
    "        if(np.linalg.norm(vect) != 0):\n",
    "            vect = vect / np.linalg.norm(vect)\n",
    "        X.append(vect)\n",
    "\n",
    "    X_train.append(X)\n",
    "\n",
    "    Y = train['root_label']\n",
    "    Y_train.append(Y)\n",
    "\n",
    "    X_test = []\n",
    "    for i in range(test.shape[0]):\n",
    "        vect = [0 for k in range(dim)]\n",
    "        keywords_str = test.iloc[i].at['keywords']\n",
    "        keywords_list = ast.literal_eval(keywords_str)\n",
    "        length = 0\n",
    "        for j in range(len(keywords_list)):\n",
    "            if bytes(keywords_list[j],'utf-8') in embeddings_dict.keys():\n",
    "                vect += embeddings_dict[bytes(keywords_list[j],'utf-8')]\n",
    "                length += 1\n",
    "        vect /= length\n",
    "        if(np.linalg.norm(vect) != 0):\n",
    "            vect = vect / np.linalg.norm(vect)\n",
    "        X_test.append(vect)\n",
    "\n",
    "    Y_test = test['root_label']\n",
    "\n",
    "\n",
    "    svm = LinearSVC(random_state=42)\n",
    "    svm.fit(X,Y)\n",
    "    pred = svm.predict(X_test)\n",
    "    print(str(dim)+\" dimension confusion matrix: \\n\", confusion_matrix(Y_test,pred))\n",
    "    print(str(dim)+\" dimension acc: \",accuracy_score(Y_test,pred))\n",
    "    print(str(dim)+\" dimension recall: \",recall_score(Y_test,pred,pos_label='sports'))\n",
    "    print(str(dim)+\" dimension precision: \",precision_score(Y_test,pred,pos_label='sports'))\n",
    "    print(str(dim)+\" dimension F-1 score: \",f1_score(Y_test,pred,pos_label='sports'))\n",
    "    if dim == 300:\n",
    "        plot_confusion_matrix(confusion_matrix(Y_test,pred),title=str(dim)+' dimension GLoVE')\n",
    "        fpr_glove, tpr_glove, _ = roc_curve(Y_test,svm.decision_function(X_test),pos_label='sports')\n",
    "        plot_roc(fpr_glove, tpr_glove)    \n",
    "    res_acc.append(accuracy_score(Y_test,pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dimension_of_glove,res_acc)\n",
    "plt.xlabel('Dimension of GLoVE embedding',fontsize=14)\n",
    "plt.ylabel('Accuracy of SVM classifier',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "reducer = UMAP(random_state=42)\n",
    "embedding = reducer.fit_transform(X_train[3])\n",
    "\n",
    "color = [0 if i == \"sports\" else 1 for i in Y_train[3]] \n",
    "\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], c=color, cmap='Spectral', s=5)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "plt.title('UMAP projection of the dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_random = np.random.rand(len(X_train[3]), len(X_train[3][0]))\n",
    "reducer = UMAP(random_state=42)\n",
    "embedding = reducer.fit_transform(X_random)\n",
    "\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], c=color, cmap='Spectral', s=5)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "plt.title('UMAP projection of random')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83893ed8ab2f7255fca62e454031b1e5cdc52831ef9a8084c39c59e4ac6d9f46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
